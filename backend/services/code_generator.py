"""Code generation service for inference scripts and deployment."""

from typing import Dict, Any
from models.schemas import GeneratedCode
from utils.logger import get_logger
from jinja2 import Template

logger = get_logger(__name__)


class CodeGenerator:
    """Service for generating deployment code and documentation."""
    
    def __init__(self):
        """Initialize code generator."""
        pass
    
    def generate_inference_script(
        self,
        model_info: Dict[str, Any],
        config: Dict[str, Any]
    ) -> GeneratedCode:
        """Generate Python inference script.
        
        Args:
            model_info: Model information
            config: Training configuration
            
        Returns:
            GeneratedCode object
        """
        logger.info(f"Generating inference script for {model_info.get('model_id')}")
        
        template = Template('''"""
Inference script for QLoRA fine-tuned {{ model_id }}
Generated by LLM Fine-Tuning Pipeline
Uses 4-bit quantization for efficient inference
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
{% if use_lora %}from peft import PeftModel, PeftConfig{% endif %}


def load_model(model_path: str, device: str = "cuda"):
    """Load the QLoRA fine-tuned model and tokenizer.
    
    Args:
        model_path: Path to the fine-tuned model (LoRA adapters)
        device: Device to load model on ('cuda' or 'cpu')
        
    Returns:
        Tuple of (model, tokenizer)
    """
    print(f"Loading tokenizer from {model_path}...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    print(f"Loading base model with QLoRA 4-bit quantization...")
    {% if quantization == "4bit" %}
    # QLoRA: Load base model with 4-bit NF4 quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_quant_type="nf4",  # NormalFloat 4-bit
        bnb_4bit_use_double_quant=True  # Nested quantization
    )
    
    {% if use_lora %}
    # Load PeftConfig to get base model name
    peft_config = PeftConfig.from_pretrained(model_path)
    
    # Load quantized base model
    model = AutoModelForCausalLM.from_pretrained(
        peft_config.base_model_name_or_path,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    
    # Load LoRA adapters
    print(f"Loading LoRA adapters from {model_path}...")
    model = PeftModel.from_pretrained(model, model_path)
    {% else %}
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map="auto",
        torch_dtype=torch.float16,
        trust_remote_code=True
    )
    {% endif %}
    {% elif quantization == "8bit" %}
    # Load with 8-bit quantization
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        load_in_8bit=True,
        device_map="auto",
        trust_remote_code=True
    )
    {% else %}
    # Load in FP16
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    {% endif %}
    
    model.eval()
    print("QLoRA model loaded successfully!")
    print(f"Model memory footprint: ~{torch.cuda.max_memory_allocated() / 1024**3:.2f} GB")
    
    return model, tokenizer


def generate_text(
    model,
    tokenizer,
    prompt: str,
    max_new_tokens: int = {{ max_seq_length }},
    temperature: float = 0.7,
    top_p: float = 0.9,
    top_k: int = 50,
    repetition_penalty: float = 1.1
) -> str:
    """Generate text from a prompt.
    
    Args:
        model: Loaded model
        tokenizer: Loaded tokenizer
        prompt: Input prompt
        max_new_tokens: Maximum tokens to generate
        temperature: Sampling temperature (higher = more random)
        top_p: Nucleus sampling parameter
        top_k: Top-k sampling parameter
        repetition_penalty: Repetition penalty
        
    Returns:
        Generated text
    """
    # Tokenize input
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    # Generate
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            repetition_penalty=repetition_penalty,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    
    # Decode
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    return generated_text


def main():
    """Example usage."""
    # Configuration
    MODEL_PATH = "./output/final_model"  # Update with your model path
    
    # Load model
    model, tokenizer = load_model(MODEL_PATH)
    
    # Example prompts
    prompts = [
        "{{ example_prompt_1 }}",
        "{{ example_prompt_2 }}"
    ]
    
    # Generate
    for prompt in prompts:
        print(f"\\n{'='*60}")
        print(f"Prompt: {prompt}")
        print(f"{'='*60}")
        
        generated = generate_text(model, tokenizer, prompt)
        print(f"Generated:\\n{generated}")


if __name__ == "__main__":
    main()
''')
        
        code = template.render(
            model_id=model_info.get('model_id', 'unknown'),
            use_lora=config.get('use_lora', True),
            quantization=config.get('quantization'),
            max_seq_length=config.get('max_seq_length', 512),
            example_prompt_1=self._get_example_prompt(config.get('task_type', 'text-generation'), 1),
            example_prompt_2=self._get_example_prompt(config.get('task_type', 'text-generation'), 2)
        )
        
        return GeneratedCode(
            code=code,
            filename="inference.py",
            language="python",
            description="Standalone inference script for the fine-tuned model"
        )
    
    def generate_gradio_app(
        self,
        model_info: Dict[str, Any],
        config: Dict[str, Any]
    ) -> GeneratedCode:
        """Generate Gradio demo application.
        
        Args:
            model_info: Model information
            config: Training configuration
            
        Returns:
            GeneratedCode object
        """
        logger.info(f"Generating Gradio app for {model_info.get('model_id')}")
        
        template = Template('''"""
Gradio Demo for fine-tuned {{ model_id }}
Generated by LLM Fine-Tuning Pipeline
"""

import torch
import gradio as gr
from transformers import AutoModelForCausalLM, AutoTokenizer
{% if use_lora %}from peft import PeftModel{% endif %}


# Global variables
model = None
tokenizer = None


def load_model(model_path: str):
    """Load the model and tokenizer."""
    global model, tokenizer
    
    print("Loading model...")
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    {% if quantization == "4bit" %}
    from transformers import BitsAndBytesConfig
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.float16
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        quantization_config=bnb_config,
        device_map="auto"
    )
    {% else %}
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    {% endif %}
    
    {% if use_lora %}
    model = PeftModel.from_pretrained(model, model_path)
    {% endif %}
    
    model.eval()
    print("Model loaded!")


def generate(
    prompt: str,
    max_tokens: int = 256,
    temperature: float = 0.7,
    top_p: float = 0.9,
    top_k: int = 50
) -> str:
    """Generate text from prompt."""
    if model is None or tokenizer is None:
        return "Error: Model not loaded. Please load a model first."
    
    try:
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                top_k=top_k,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return generated_text
        
    except Exception as e:
        return f"Error during generation: {str(e)}"


# Create Gradio interface
def create_interface():
    """Create Gradio UI."""
    
    with gr.Blocks(title="{{ model_id }} - Fine-tuned Model Demo") as demo:
        gr.Markdown("# ðŸ¤– {{ model_id }} - Fine-tuned Model")
        gr.Markdown("Interactive demo for your fine-tuned language model")
        
        with gr.Row():
            with gr.Column():
                prompt_input = gr.Textbox(
                    label="Prompt",
                    placeholder="Enter your prompt here...",
                    lines=5
                )
                
                with gr.Row():
                    max_tokens = gr.Slider(
                        minimum=50,
                        maximum=1024,
                        value=256,
                        step=50,
                        label="Max Tokens"
                    )
                    temperature = gr.Slider(
                        minimum=0.1,
                        maximum=2.0,
                        value=0.7,
                        step=0.1,
                        label="Temperature"
                    )
                
                with gr.Row():
                    top_p = gr.Slider(
                        minimum=0.1,
                        maximum=1.0,
                        value=0.9,
                        step=0.05,
                        label="Top P"
                    )
                    top_k = gr.Slider(
                        minimum=1,
                        maximum=100,
                        value=50,
                        step=1,
                        label="Top K"
                    )
                
                generate_btn = gr.Button("Generate", variant="primary")
            
            with gr.Column():
                output = gr.Textbox(
                    label="Generated Text",
                    lines=15
                )
        
        # Examples
        gr.Examples(
            examples=[
                ["{{ example_prompt_1 }}"],
                ["{{ example_prompt_2 }}"],
                ["{{ example_prompt_3 }}"]
            ],
            inputs=[prompt_input]
        )
        
        # Connect button
        generate_btn.click(
            fn=generate,
            inputs=[prompt_input, max_tokens, temperature, top_p, top_k],
            outputs=output
        )
    
    return demo


def main():
    """Main function."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Gradio demo for fine-tuned model")
    parser.add_argument("--model-path", type=str, required=True, help="Path to model")
    parser.add_argument("--share", action="store_true", help="Create public link")
    parser.add_argument("--port", type=int, default=7860, help="Port number")
    
    args = parser.parse_args()
    
    # Load model
    load_model(args.model_path)
    
    # Create and launch interface
    demo = create_interface()
    demo.launch(server_name="0.0.0.0", server_port=args.port, share=args.share)


if __name__ == "__main__":
    main()
''')
        
        code = template.render(
            model_id=model_info.get('model_id', 'unknown'),
            use_lora=config.get('use_lora', True),
            quantization=config.get('quantization'),
            example_prompt_1=self._get_example_prompt(config.get('task_type', 'text-generation'), 1),
            example_prompt_2=self._get_example_prompt(config.get('task_type', 'text-generation'), 2),
            example_prompt_3=self._get_example_prompt(config.get('task_type', 'text-generation'), 3)
        )
        
        return GeneratedCode(
            code=code,
            filename="gradio_app.py",
            language="python",
            description="Interactive Gradio web application for model inference"
        )
    
    def generate_api_wrapper(
        self,
        model_info: Dict[str, Any],
        config: Dict[str, Any]
    ) -> GeneratedCode:
        """Generate FastAPI inference endpoint.
        
        Args:
            model_info: Model information
            config: Training configuration
            
        Returns:
            GeneratedCode object
        """
        logger.info(f"Generating API wrapper for {model_info.get('model_id')}")
        
        template = Template('''"""
FastAPI inference endpoint for {{ model_id }}
Generated by LLM Fine-Tuning Pipeline
"""

import torch
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from transformers import AutoModelForCausalLM, AutoTokenizer
{% if use_lora %}from peft import PeftModel{% endif %}
from typing import Optional
import uvicorn


# Request/Response models
class GenerateRequest(BaseModel):
    """Request model for text generation."""
    prompt: str = Field(..., description="Input prompt")
    max_tokens: int = Field(256, ge=1, le=2048, description="Maximum tokens to generate")
    temperature: float = Field(0.7, ge=0.1, le=2.0, description="Sampling temperature")
    top_p: float = Field(0.9, ge=0.0, le=1.0, description="Nucleus sampling")
    top_k: int = Field(50, ge=1, le=100, description="Top-k sampling")
    repetition_penalty: float = Field(1.1, ge=1.0, le=2.0)


class GenerateResponse(BaseModel):
    """Response model for text generation."""
    generated_text: str
    prompt: str
    tokens_generated: int


# Global model state
app = FastAPI(
    title="{{ model_id }} Inference API",
    description="API for fine-tuned model inference",
    version="1.0.0"
)

model = None
tokenizer = None


@app.on_event("startup")
async def load_model():
    """Load model on startup."""
    global model, tokenizer
    
    MODEL_PATH = "./output/final_model"  # Configure this
    
    print("Loading model...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    
    {% if quantization == "4bit" %}
    from transformers import BitsAndBytesConfig
    bnb_config = BitsAndBytesConfig(load_in_4bit=True)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        quantization_config=bnb_config,
        device_map="auto"
    )
    {% else %}
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    {% endif %}
    
    {% if use_lora %}
    model = PeftModel.from_pretrained(model, MODEL_PATH)
    {% endif %}
    
    model.eval()
    print("Model loaded successfully!")


@app.get("/")
async def root():
    """Root endpoint."""
    return {
        "message": "{{ model_id }} Inference API",
        "version": "1.0.0",
        "endpoints": ["/generate", "/health"]
    }


@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {
        "status": "healthy",
        "model_loaded": model is not None
    }


@app.post("/generate", response_model=GenerateResponse)
async def generate_text(request: GenerateRequest):
    """Generate text from prompt.
    
    Args:
        request: Generation request
        
    Returns:
        Generated text response
    """
    if model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    try:
        # Tokenize
        inputs = tokenizer(request.prompt, return_tensors="pt").to(model.device)
        input_length = inputs.input_ids.shape[1]
        
        # Generate
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens,
                temperature=request.temperature,
                top_p=request.top_p,
                top_k=request.top_k,
                repetition_penalty=request.repetition_penalty,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # Decode
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        tokens_generated = outputs.shape[1] - input_length
        
        return GenerateResponse(
            generated_text=generated_text,
            prompt=request.prompt,
            tokens_generated=tokens_generated
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Generation failed: {str(e)}")


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
''')
        
        code = template.render(
            model_id=model_info.get('model_id', 'unknown'),
            use_lora=config.get('use_lora', True),
            quantization=config.get('quantization')
        )
        
        return GeneratedCode(
            code=code,
            filename="api_server.py",
            language="python",
            description="FastAPI server for model inference"
        )
    
    def generate_readme(
        self,
        model_info: Dict[str, Any],
        config: Dict[str, Any],
        training_summary: Dict[str, Any]
    ) -> GeneratedCode:
        """Generate comprehensive README.
        
        Args:
            model_info: Model information
            config: Training configuration
            training_summary: Training results summary
            
        Returns:
            GeneratedCode object
        """
        logger.info("Generating README")
        
        template = Template('''# {{ model_id }} - Fine-Tuned Model

Fine-tuned version of [{{ model_id }}](https://huggingface.co/{{ model_id }}) using the LLM Fine-Tuning Pipeline.

## Model Details

- **Base Model:** {{ model_id }}
- **Parameters:** {{ num_parameters }}
- **Architecture:** {{ architecture }}
- **Task:** {{ task_type }}
- **Fine-tuning Method:** {% if use_lora %}LoRA (Low-Rank Adaptation){% else %}Full Fine-tuning{% endif %}
{% if quantization %}- **Quantization:** {{ quantization }}{% endif %}

## Training Details

### Dataset
- **Samples:** {{ num_samples }}
- **Average Tokens:** {{ avg_tokens }}

### Hyperparameters
```yaml
learning_rate: {{ learning_rate }}
batch_size: {{ batch_size }}
gradient_accumulation_steps: {{ gradient_accumulation_steps }}
epochs: {{ num_epochs }}
optimizer: {{ optimizer }}
scheduler: {{ scheduler }}
{% if use_lora %}
lora_r: {{ lora_r }}
lora_alpha: {{ lora_alpha }}
lora_dropout: {{ lora_dropout }}
{% endif %}
```

### Training Results
- **Final Train Loss:** {{ final_train_loss }}
- **Training Time:** {{ training_time }}
- **GPU Used:** {{ gpu_type }}

## Installation

```bash
pip install torch transformers {% if use_lora %}peft {% endif %}{% if quantization %}bitsandbytes {% endif %}accelerate
```

## Usage

### Basic Inference

```python
from transformers import AutoModelForCausalLM, AutoTokenizer
{% if use_lora %}from peft import PeftModel{% endif %}
import torch

# Load model and tokenizer
tokenizer = AutoTokenizer.from_pretrained("./model")
model = AutoModelForCausalLM.from_pretrained(
    "./model",
    torch_dtype=torch.float16,
    device_map="auto"
)

# Generate text
prompt = "{{ example_prompt }}"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=256)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(generated_text)
```

### Using the Inference Script

```bash
python inference.py
```

### Using the Gradio App

```bash
python gradio_app.py --model-path ./model --share
```

### Using the API Server

```bash
python api_server.py
```

Then make requests:
```bash
curl -X POST "http://localhost:8000/generate" \\
  -H "Content-Type: application/json" \\
  -d '{"prompt": "Your prompt here", "max_tokens": 256}'
```

## Performance

| Metric | Value |
|--------|-------|
| Parameters | {{ num_parameters }} |
| VRAM (Inference) | {{ vram_inference }} GB |
| VRAM (Training) | {{ vram_training }} GB |
| Inference Speed | {{ inference_speed }} tokens/sec |

## Files

```
.
â”œâ”€â”€ config.json              # Model configuration
â”œâ”€â”€ pytorch_model.bin        # Model weights
â”œâ”€â”€ tokenizer_config.json    # Tokenizer configuration
â”œâ”€â”€ special_tokens_map.json  # Special tokens
â”œâ”€â”€ inference.py             # Inference script
â”œâ”€â”€ gradio_app.py           # Gradio demo
â”œâ”€â”€ api_server.py           # FastAPI server
â””â”€â”€ README.md               # This file
```

## License

This model is based on {{ model_id }} and inherits its license.

## Citation

If you use this model, please cite:

```bibtex
@misc{fine-tuned-{{ model_id_safe }},
  author = {Your Name},
  title = {Fine-tuned {{ model_id }}},
  year = {2025},
  publisher = {GitHub/HuggingFace},
  howpublished = {\\url{https://your-url-here}}
}
```

## Acknowledgments

- Base model: [{{ model_id }}](https://huggingface.co/{{ model_id }})
- Fine-tuning pipeline: LLM Fine-Tuning Automation Pipeline
- Framework: Hugging Face Transformers{% if use_lora %}, PEFT{% endif %}

## Contact

For questions or issues, please open an issue on GitHub.
''')
        
        code = template.render(
            model_id=model_info.get('model_id', 'unknown'),
            model_id_safe=model_info.get('model_id', 'unknown').replace('/', '-'),
            num_parameters=model_info.get('parameter_size', 'unknown'),
            architecture=model_info.get('architecture', 'unknown'),
            task_type=config.get('task_type', 'text-generation'),
            use_lora=config.get('use_lora', True),
            quantization=config.get('quantization', 'None'),
            num_samples=training_summary.get('num_samples', 'N/A'),
            avg_tokens=training_summary.get('avg_tokens', 'N/A'),
            learning_rate=config.get('learning_rate', '2e-4'),
            batch_size=config.get('batch_size', 4),
            gradient_accumulation_steps=config.get('gradient_accumulation_steps', 4),
            num_epochs=config.get('num_epochs', 3),
            optimizer=config.get('optimizer', 'adamw'),
            scheduler=config.get('scheduler', 'cosine'),
            lora_r=config.get('lora_config', {}).get('r', 8),
            lora_alpha=config.get('lora_config', {}).get('lora_alpha', 16),
            lora_dropout=config.get('lora_config', {}).get('lora_dropout', 0.05),
            final_train_loss=training_summary.get('final_loss', 'N/A'),
            training_time=training_summary.get('training_time', 'N/A'),
            gpu_type=training_summary.get('gpu_type', 'Unknown'),
            example_prompt=self._get_example_prompt(config.get('task_type', 'text-generation'), 1),
            vram_inference=model_info.get('vram_requirements', {}).get('inference_fp16', 'N/A'),
            vram_training=model_info.get('vram_requirements', {}).get('training_lora_fp16', 'N/A'),
            inference_speed=training_summary.get('inference_speed', 'N/A')
        )
        
        return GeneratedCode(
            code=code,
            filename="README.md",
            language="markdown",
            description="Comprehensive documentation for the fine-tuned model"
        )
    
    def _get_example_prompt(self, task_type: str, variant: int = 1) -> str:
        """Get example prompt for task type.
        
        Args:
            task_type: Task type
            variant: Prompt variant number
            
        Returns:
            Example prompt
        """
        prompts = {
            "text-generation": [
                "Once upon a time in a land far away,",
                "The future of artificial intelligence is",
                "Explain quantum computing in simple terms:"
            ],
            "text-classification": [
                "This movie was absolutely fantastic!",
                "I'm not sure how I feel about this product.",
                "Worst experience ever. Would not recommend."
            ],
            "summarization": [
                "Summarize the following article: ...",
                "Provide a brief summary of: ...",
                "TL;DR: ..."
            ],
            "question-answering": [
                "Question: What is the capital of France?",
                "Q: How does photosynthesis work?",
                "Answer this question: ..."
            ]
        }
        
        task_prompts = prompts.get(task_type, prompts["text-generation"])
        return task_prompts[variant % len(task_prompts)]
